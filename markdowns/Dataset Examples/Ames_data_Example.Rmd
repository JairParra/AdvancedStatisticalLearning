---
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{amsthm}
title: "2. Ames Data Example"
author: "Hair Parra"
date: "`r Sys.Date()`"
geometry: margin=1.2cm
output: 
    pdf_document: 
      extra_dependencies: ["array", "amsmath","booktabs"]
---

\newtheorem{assumption}{Assumption}[assumption]
\newtheorem{theorem}{Theorem}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{example}{Example}
\newtheorem{remark*}{Remark}
\newtheorem{exercise*}{Exercise}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=9, fig.height=6) 

# configurations for plot 
my_plot_hook <- function(x, options)
  paste("\n", knitr::hook_plot_tex(x, options), "\n")
knitr::knit_hooks$set(plot = my_plot_hook)
```

## Libraries 

```{r, message=FALSE, warning=FALSE}
library("glmnet")
```



# Ames Data Example 

- alternative to the well-known Boston Housing data set.
- It has 2,330 observations and 82 variables and contains information from the Ames Assessor's Office used in computing assessed values for individual residential properties sold in Ames, IA from 2006 to 2010. 
- https://ww2.amstat.org/publications/jse/v19n3/decock/DataDocumentation.txt

## Description 

- The data has 82 columns which include 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers).
- The usual goal is to predict the `SalePrice` of a house.
- The package `AmesHousing` contains the raw data and also pre-processed versions of them. 


## Pre-processing 

First construct a function that will be used to combine together levels with few observations, for a factor variable.


```{r}
# Function to combine levels, with less than a specified number of observations, of a factor variable
# Inputs:
#   x = variable (must be a factor; if not, the original variable is returned)
#   nmin = levels with fewer than nmin observations will be combined
# Output:
#   Returns the same variable with a new level called "othcomb" replacing the combined levels (NA's are not affected)
comblev <- function(x, nmin) {
  if (!is.factor(x)) {
    return(x)
  }
  
  # Load the 'rockchalk' library for the 'combineLevels' function
  library(rockchalk)
  
  # Create a frequency table of factor levels
  ta <- table(x)
  
  # Combine levels with fewer than 'nmin' observations into a new level "othcomb"
  combineLevels(x, levs = names(table(x))[table(x) < nmin], newLabel = c("othcomb"))
}


# to apply it to a data frame "mat" and get a data frame as the result
# data.frame(lapply(mat,comblev,nmin=2))
```

### Prepare the ames housing data set.

- This version of the data uses the ordered factor as numeric variables and the non-ordered factor are left as factors, but the levels with less than 30 observations are combined.
- In the end, we have **22** factors and **57** numeric covariates, and **1 target** "Sale_Price".


```{r}
# load the library 
library(AmesHousing)

# load the data in ordinal form 
ames=make_ordinal_ames() 
ames
```

```{r}
colnames(ames)
```


```{r}
# remove an observation with a missing
ames=ames[!is.na(ames$Electrical),]

# remove the variable "Utilities" because it is almost constant 
# with frequencies (1,1,2909).  
ames$Utilities=NULL

# converts the target variable (1 = 1000)
ames$Sale_Price=ames$Sale_Price/1000

# get the names of the ordinal variables
ord_vars=vapply(ames, is.ordered, logical(1))

# converts the ordered factors to numeric (this preserves the ordering of the factor)
namored=names(ord_vars)[ord_vars]
ames[,namored]=data.frame(lapply(ames[,namored],as.numeric))

# get the names of the factor variables
fac_vars=vapply(ames, is.factor, logical(1))
namfac=names(fac_vars)[fac_vars]

# display the factor variables
head(namfac)
```

```{r}
# group together levels with less than 30 observations
ames=data.frame(lapply(ames,comblev,nmin=30))

# remove the space in the values (string) of some variables to prevent problems later
ames[,"Exterior_1st"]=as.factor(gsub(" ","",ames[,"Exterior_1st"]))
ames[,"Exterior_2nd"]=as.factor(gsub(" ","",ames[,"Exterior_2nd"])) 

# get the names of the factor variables
num_vars=vapply(ames, is.numeric, logical(1)) 

# names of the numeric variables
namnum=names(num_vars)[num_vars]
```
```{r}
head(ames)
```


- This version of the data set, `ames`, has **2929 observations** and contains **22 factors** and **58 numeric covariates**, including **1 target Sale Price**.
- As explained in the `make_ames` function documentation, some observations and variables were removed, and 2 new variables were added.
- The factors have been consolidated. All levels **with less than 30 observations** are grouped together.

### ANother version of the data: `amesdum` 

We also prepare another version of the data, `amesdum` where the **factor variables are replaced by dummy variables**.


```{r}
# Load the 'fastDummies' library for creating dummy variables
library(fastDummies)

# Create dummy variables for the factors in the 'ames' dataset
amesdum = dummy_cols(ames, remove_first_dummy = TRUE, remove_selected_columns = TRUE)

# Check the dimensions of the 'amesdum' dataset
# It has 2929 rows and 161 columns (160 covariates and 1 target "Sale_Price")
dim(amesdum)

# Display a summary of the 'amesdum' dataset
# summary(amesdum)

head(amesdum)
```

### Train-test split 

```{r}
# Splitting the data into a training (ntrain=1000) and a test (ntest=1929) set

# Set a random seed for reproducibility
set.seed(489565)

# Define the number of training samples (ntrain) and test samples (ntest)
ntrain = 1000
ntest = nrow(ames) - ntrain

# Randomly select 'ntrain' indices for the training set without replacement
indtrain = sample(1:nrow(ames), ntrain, replace = FALSE)

# Create a copy of 'amesdum' and remove the 'Sale_Price' column
xdum = amesdum
xdum$Sale_Price = NULL
xdum = as.matrix(xdum)

# Create separate datasets for training and testing
amestrain = ames[indtrain,]
amestest = ames[-indtrain,]
amesdumtrain = amesdum[indtrain,]
amesdumtest = amesdum[-indtrain,]
xdumtrain = xdum[indtrain,]
xdumtest = xdum[-indtrain,]
```


## Data Analysis 

1. Next is first a simple wrapper function to apply `glmnet` and get the predictions and `coeficients` for the tuning parameter with minimum CV error, and with the 1 SE rule.
2. It also computes the MAE and MSE
3. It also produces some plots


```{r}
# Variable selection examples, with all 161 covariates
# (including the dummies for the categorical covariates)

# Function to apply glmnet and get predictions, coefficients, and errors
# - xtrain: training data predictors
# - ytrain: training data target
# - xtest: test data predictors
# - ytest: test data target (optional)
# - alpha: alpha parameter for glmnet

wrapglmnet = function(xtrain, ytrain, xtest, ytest = NULL, alpha) {
  require(glmnet)
  
  # Set the layout for multiple plots
  par(mfrow = c(2, 2))
  
  # Plot the glmnet results
  plot(glmnet(x = xtrain, y = ytrain, alpha = alpha), xvar = "lambda", label = TRUE)
  
  # Cross-validate and plot the results
  cv = cv.glmnet(x = xtrain, y = ytrain, alpha = alpha)
  plot(cv)
  
  # Predict using lambda.min and lambda.1se
  pred = predict(cv, new = xtest, s = "lambda.min")
  pred1se = predict(cv, new = xtest, s = "lambda.1se") # this is calculate automatically by cv.glmnet
  
  # Initialize error metrics
  err = NA
  
  # If ytest is available, compute MAE and MSE
  if (!is.null(ytest)) {
    # plot predictions 
    plot(ytest, pred)
    plot(ytest, pred1se) 
    
    # calculate errors for each framework 
    err = data.frame(
      mean(abs(pred - ytest)), mean((pred - ytest)^2),
      mean(abs(pred1se - ytest)), mean((pred1se - ytest)^2)
    )
    names(err) = c("MAE", "MSE", "MAE_1SE", "MSE_1SE")
  }
  
  # Get coefficients for lambda.min and lambda.1se
  co = predict(cv, s = "lambda.min", type = "coefficients")
  co = as.matrix(co)
  co = co[co[, 1] != 0,, drop = FALSE]
  
  co1se = predict(cv, s = "lambda.1se", type = "coefficients")
  co1se = as.matrix(co1se)
  co1se = co1se[co1se[, 1] != 0,, drop = FALSE]
  
  # Create a list of results
  out = list(err, co, co1se, pred, pred1se)
  names(out) = c("error", "coef", "coef1se", "pred", "pred1se")
  
  # Return the results
  out
}
```


### Lasso 

```{r, fig.width=9, fig.height=8}
# basic lasso
las=wrapglmnet(xdumtrain, # xtrain
               amesdumtrain$Sale_Price, # ytrain  
               xdumtest, # xtest
               amesdumtest$Sale_Price, # ytest
               1) # alpha glmnet 
```

```{r}
# number of coefficients with 1SE vs min lambda
dim(las$coef1se)
dim(las$coef)
```

```{r}
# COefficients for 1SE rule 
las$coef1se
```

```{r}
# coefficients without the rule 
las$coef
```

```{r}
# Error metrics for each method 
las$err
```

- We see the 1 SE rule selects 19 variables, while the lasso retains 65 variables. 
- However, the MAE and the MSE are bigger (more explainable model but less predictive power).

### Ridge 

```{r, fig.width=9, fig.height=8}
# basic ridge
rid=wrapglmnet(xdumtrain,
               amesdumtrain$Sale_Price,
               xdumtest,
               amesdumtest$Sale_Price,
               0) # alpha glmnet: ridge
``` 

```{r}
dim(rid$coef1se)
dim(rid$coef)
```

```{r}
rid$err
```

```{r}
las$err
```


- RIdge does not perform variable selection. 
- The MSE of Ridge is very close to the one of lasso 

## OLS Regression 

```{r}
# Ordinary OLS regression

# Fit an Ordinary Least Squares (OLS) regression model using all covariates
lmfit = lm(Sale_Price ~ ., data = amesdumtrain)

# Make predictions on the test data using the OLS model
predlmfit = predict(lmfit, newdata = amesdumtest)

# Compute Mean Absolute Error (MAE) and Mean Squared Error (MSE) for the OLS model
errlmfit = data.frame(
  mean(abs(predlmfit - amesdumtest$Sale_Price)),
  mean((predlmfit - amesdumtest$Sale_Price)^2)
)

# Rename the columns in the error data frame
names(errlmfit) = c("MAE", "MSE")

# Display the MAE and MSE for the OLS model
errlmfit
```

We see that the OLS does not do as well as lasso and ridge. 


## Refitting the Lasso 

We could fit an ordinary OLS to the variables selected by the lasso, or run the lasso again on them.

### Lasso + OLS

```{r}
# Fit an OLS model using only the lasso-selected variables

# Extract the names of lasso-selected variables (excluding intercept)
namlas = rownames(las$coef)[-1]

# Fit an Ordinary Least Squares (OLS) model using the lasso-selected variables and Sale_Price
laslm = lm(Sale_Price ~ ., data = amesdumtrain[, c(namlas, "Sale_Price")])

# Make predictions on the test data using the OLS model with lasso-selected variables
predlaslm = predict(laslm, newdata = amesdumtest)

# Compute Mean Absolute Error (MAE) and Mean Squared Error (MSE) for the OLS model with lasso-selected variables
errlaslm = data.frame(
  mean(abs(predlaslm - amesdumtest$Sale_Price)),
  mean((predlaslm - amesdumtest$Sale_Price)^2)
)

# Rename the columns in the error data frame
names(errlaslm) = c("MAE", "MSE")

# Display the MAE and MSE for the OLS model with lasso-selected variables
errlaslm
```

### Lasso + Lasso

```{r, fig.width=9, fig.height=8}
# Apply the lasso regression again to the lasso-selected variables only

# Use the 'wrapglmnet' function to apply lasso regression
laslas = wrapglmnet(
  xdumtrain[, namlas], 
  amesdumtrain$Sale_Price,
  xdumtest[, namlas], 
  amesdumtest$Sale_Price, 
  1 # alpha parameter
)
```

```{r}
# Check if all variables are kept in the lasso-lasso solution
rownames(laslas$coef) == rownames(las$coef)
```


```{r}
# Calculate the proportion of parameters greater in magnitude in the lasso-lasso solution compared to the lasso
# mean(abs(laslas$coef) > abs(las$coef))

# Get the variable names from both 'las' and 'laslas' models
common_vars <- intersect(rownames(las$coef), rownames(laslas$coef))

# Calculate the proportion of parameters with greater magnitude in 'laslas' compared to 'las'
prop_greater_magnitude <- mean(abs(laslas$coef[common_vars, ]) > abs(las$coef[common_vars, ]))

# Display the proportion
prop_greater_magnitude

```

```{r}
# Display the error metrics from the lasso-lasso solution
laslas$er
```

## Relaxed Lasso 

We could use the relaxed lasso using the option `relax=TRUE`. Here, an extra tuning parameter (noted $\gamma$) must be estimated.


```{r, fig.width=9, fig.height=6}
# The right code for the relax lasso
library("glmnet")

# Perform cross-validation with relax lasso using cv.glmnet
# cv.relax = cv.glmnet(x = xtrain, y = ytrain, alpha = 1, relax = TRUE)
cv.relax = cv.glmnet(x = xdumtrain, y = amesdumtrain$Sale_Price, alpha = 1, relax = TRUE)

# Plot the cross-validation results for relax lasso
plot(cv.relax)
```



```{r}
# Predictions with optimal lambda and gamma for the relax lasso

# Make predictions using lambda.min and gamma.min for relax lasso
pred = predict(cv.relax, new = xdumtest, s = "lambda.min", gamma = "gamma.min")
pred1se = predict(cv.relax, new = xdumtest, s = "lambda.1se", gamma = "gamma.1se")

# Compute error metrics for the relax lasso predictions
errrla = data.frame(
  MAE = mean(abs(pred - amesdumtest$Sale_Price)),
  MSE = mean((pred - amesdumtest$Sale_Price)^2),
  MAE_1SE = mean(abs(pred1se - amesdumtest$Sale_Price)),
  MSE_1SE = mean((pred1se - amesdumtest$Sale_Price)^2)
)

# Display the error metrics for the relax lasso predictions
errrla
```

Finally, we could perform relaxed elastic net, but we need to fix the value of $\alpha$, which is not optimized in glmnet using CV. 

```{r}
# Putting all the results together and sorting them according to the MAE and MSE

# Combine error metrics from different models
allres = rbind(
  las$err[, 1:2],         # Lasso regression
  rid$err[, 1:2],         # Ridge regression
  errlmfit,               # Ordinary Least Squares (OLS)
  errlaslm,               # Lasso regression with selected variables
  laslas$err[, 1:2],      # Lasso-lasso regression
  errrla[, 1:2]           # Relaxed Lasso regression
)

# Assign row names to the combined results
row.names(allres) = c("lasso", "ridge", "OLS", "lasso-OLS", "lasso-lasso", "relaxed lasso")

# Sort the results by MAE
sorted_by_MAE = allres[order(allres[, 1]), ]

# Sort the results by MSE
sorted_by_MSE = allres[order(allres[, 2]), ]

# Display the sorted results by MAE and MSE
sorted_by_MAE
sorted_by_MSE
```

We see that the ridge did the best according to MAE and MSE. 


## Group Lasso and Exponential Lasso 


### Group Lasso 

$$
\begin{aligned}
\hat{\beta} &= \arg\min_{\beta} \left\{ \sum_{i=1}^{n} (y_i - (\beta_0 + \beta^T x_i))^2 + \lambda \sum_{\ell=1}^{L} \sqrt{p_{\ell}} \left( \sum_{j \in G_{\ell}} \beta_j^2 \right)^{\frac{1}{2}}   \right\} 
\end{aligned}
$$

```{r}
# Load the 'grpreg' library
library(grpreg)

# Treating the dummies of a categorical variable as a group

# Define the 'group' vector for grpreg
group <- c(1:57, rep(58, 11), rep(59, 3), 60, rep(61, 2), rep(62, 4), rep(63, 21), rep(64, 5),
           65, rep(66, 4), rep(67, 5), rep(68, 2), 69, rep(70, 10), rep(71, 10),
           rep(72, 3), rep(73, 4), 74, 75, rep(76, 5), rep(77, 2), rep(78, 3), rep(79, 4))

# Set a random seed for reproducibility
set.seed(14273)

# Fit a group lasso regression model
grlassofit <- grpreg(xdumtrain, amesdumtrain$Sale_Price, group, penalty = "grLasso")

# Plot the results of the group lasso regression
plot(grlassofit)
```

```{r}
# Group lasso with cross-validation
grlassofitcv <- cv.grpreg(xdumtrain, amesdumtrain$Sale_Price, group, seed = 474659, penalty = "grLasso")

# Predict coefficients and test data
coefgrlasso <- predict(grlassofitcv, type = "coefficients")
predgrlasso <- predict(grlassofitcv, X = xdumtest)

# Calculate MAE and MSE
errgrlasso <- data.frame(
  MAE = mean(abs(predgrlasso - amesdumtest$Sale_Price)),
  MSE = mean((predgrlasso - amesdumtest$Sale_Price)^2)
)
names(errgrlasso) <- c("MAE", "MSE")
row.names(errgrlasso) <- c("group lasso")

# Display the error metrics
errgrlasso

# Append the results to the 'allres' dataframe
allres <- rbind(allres, errgrlasso)
```

### Exponential Lasso 

```{r} 
# random seed for replication 
set.seed(73888)

# Exponential lasso with cross-validation
gelcv <- cv.grpreg(xdumtrain, amesdumtrain$Sale_Price, group, seed = 474659, penalty = "gel")

# Predict coefficients and test data
coefgel <- predict(gelcv, type = "coefficients")
predgel <- predict(gelcv, X = xdumtest)

# Calculate MAE and MSE
errgel <- data.frame(
  MAE = mean(abs(predgel - amesdumtest$Sale_Price)),
  MSE = mean((predgel - amesdumtest$Sale_Price)^2)
)
names(errgel) <- c("MAE", "MSE")
row.names(errgel) <- c("exponential lasso")

# Display the error metrics
errgel

# Append the results to the 'allres' dataframe
allres <- rbind(allres, errgel)
```

```{r}
# Best methods based on MAE and MSE
allres[order(allres[, 1]), ]
allres[order(allres[, 2]), ]
```

**Note:** For predictive purposes, these methods do not have generally disadvantage over the ones that treat the dummies as individual variables. 





# Tree-based Methods

## CART on Ames Data 

```{r}
# Load the 'rpart' library for recursive partitioning and regression trees
library(rpart)

# Set the random seed for reproducibility
set.seed(37569)

# Fit a regression tree model to predict 'Sale_Price' using all predictors
rptree <- rpart(Sale_Price ~ ., data = amestrain, method = "anova",
                control = rpart.control(xval = 10, minsplit = 10, minbucket = 3, cp = 0))

# Prune the regression tree to improve its generalization performance
rptreepruned <- prune(rptree, cp = rptree$cp[which.min(rptree$cp[,"xerror"]),"CP"])

# Predict 'Sale_Price' for the test data using the pruned regression tree
predrpart <- predict(rptreepruned, newdata = amestest)

# Calculate the mean absolute error (MAE) and mean squared error (MSE) of the predictions
errrpart <- data.frame(mean(abs(predrpart - amesdumtest$Sale_Price)),
                       mean((predrpart - amesdumtest$Sale_Price)^2))

# Set column and row names for the error data frame
names(errrpart) <- c("MAE", "MSE")
row.names(errrpart) <- c("single tree (rpart)")

# Display the error metrics
errrpart
```


```{r}
# Append the error metrics from 'errrpart' to the 'allres' data frame
allres <- rbind(allres, errrpart)

# Sort the 'allres' data frame by MSE in increasing order
allres <- allres[order(allres$MSE), ]

# Display the updated 'allres' data frame
allres
```


```{r, message=FALSE}
# Set the random seed for reproducibility
set.seed(33967)

# Load the 'randomForest' library for random forest regression
library(randomForest)

# Fit a random forest regression model to predict 'Sale_Price' using all predictors
rf <- randomForest(Sale_Price ~ ., data = amestrain, ntree = 500)

# Predict 'Sale_Price' for the test data using the random forest model
predrf <- predict(rf, newdata = amestest)

# Calculate the mean absolute error (MAE) and mean squared error (MSE) of the predictions
errrf <- data.frame(mean(abs(predrf - amesdumtest$Sale_Price)),
                    mean((predrf - amesdumtest$Sale_Price)^2))

# Set column and row names for the error data frame
names(errrf) <- c("MAE", "MSE")
row.names(errrf) <- c("random forest")

# Display the error metrics
errrf
```
```{r}
# Append the error metrics from 'errrf' to the 'allres' data frame
allres <- rbind(allres, errrf)

# Sort the 'allres' data frame by MSE in increasing order
allres <- allres[order(allres$MSE), ]

# Display the updated 'allres' data frame
allres
```


### VIMP measures 

We continue the Ames data example by computing the VIMP with `randomForest`.

```{r}
# retrieve variable importance
virf=importance(rf)
virf
```

```{r}
varImpPlot(rf)
```








